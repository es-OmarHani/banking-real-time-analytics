services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.4.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://127.0.0.1:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1

      # helps during dev
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"

  postgres:
    image: postgres:15
    container_name: postgres
    env_file:
      - ./.env
    environment:
      POSTGRES_HOST_AUTH_METHOD: scram-sha-256
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256"
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    command: >
      postgres -c wal_level=logical
              -c max_wal_senders=10
              -c max_replication_slots=10
              -c password_encryption=scram-sha-256
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 10s
      timeout: 5s
      retries: 5

  connect:
    image: quay.io/debezium/connect:3.4
    container_name: connect
    depends_on:
      kafka:
        condition: service_started
      postgres:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: "kafka:9092"
      GROUP_ID: "1"
      CONFIG_STORAGE_TOPIC: "connect-configs"
      OFFSET_STORAGE_TOPIC: "connect-offsets"
      STATUS_STORAGE_TOPIC: "connect-status"
      CONFIG_STORAGE_REPLICATION_FACTOR: "1"
      OFFSET_STORAGE_REPLICATION_FACTOR: "1"
      STATUS_STORAGE_REPLICATION_FACTOR: "1"
      KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      VALUE_CONVERTER_SCHEMAS_ENABLE: "false"

  minio:
    image: minio/minio:latest
    container_name: minio
    command: server /data --console-address ":9001"
    env_file:
      - ./.env
    ports:
      - "9002:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data

  airflow-postgres:
    image: postgres:15
    container_name: airflow-postgres
    restart: always
    env_file:
      - ./.env
    environment:
      POSTGRES_USER: ${AIRFLOW_DB_USER}
      POSTGRES_PASSWORD: ${AIRFLOW_DB_PASSWORD}
      POSTGRES_DB: ${AIRFLOW_DB_NAME}
    volumes:
      - airflow_postgres_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${AIRFLOW_DB_USER} -d ${AIRFLOW_DB_NAME}"]
      interval: 10s
      timeout: 5s
      retries: 10

  airflow-init:
    build:
      context: .
      dockerfile: dockerfile-airflow.dockerfile
    container_name: airflow-init
    depends_on:
      airflow-postgres:
        condition: service_healthy
    restart: "no"
    env_file:
      - ./.env
      - ./docker/dags/.env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager

      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-postgres:5432/${AIRFLOW_DB_NAME}

      # Airflow 3 Execution API base url (SDK)
      AIRFLOW__SDK__API_BASE_URL: http://airflow-api-server:8080
    volumes:
      - ./dags:/opt/airflow/dags
      - ./docker/logs:/opt/airflow/logs
      - ./docker/plugins:/opt/airflow/plugins
    entrypoint: /bin/bash
    command:
      - -c
      - |
        set -e
        airflow db migrate
        airflow users create \
          --username "${AIRFLOW_ADMIN_USER}" \
          --password "${AIRFLOW_ADMIN_PASSWORD}" \
          --firstname "${AIRFLOW_ADMIN_FIRSTNAME}" \
          --lastname "${AIRFLOW_ADMIN_LASTNAME}" \
          --role "Admin" \
          --email "${AIRFLOW_ADMIN_EMAIL}" || true
        echo "Airflow initialization completed!"

  airflow-api-server:
    build:
      context: .
      dockerfile: dockerfile-airflow.dockerfile
    container_name: airflow-api-server
    restart: always
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    env_file:
      - ./.env
      - ./docker/dags/.env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager

      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-postgres:5432/${AIRFLOW_DB_NAME}
      AIRFLOW__SDK__API_BASE_URL: http://airflow-api-server:8080

      # session auth for UI
      AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.session
    volumes:
      - ./dags:/opt/airflow/dags
      - ./docker/logs:/opt/airflow/logs
      - ./docker/plugins:/opt/airflow/plugins
    ports:
      - "8080:8080"
    command: api-server
    healthcheck:
      test:
        [
          "CMD-SHELL",
          "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8080/api/v2/monitor/health').read()\""
        ]
      interval: 30s
      timeout: 10s
      retries: 10

  airflow-dag-processor:
    build:
      context: .
      dockerfile: dockerfile-airflow.dockerfile
    container_name: airflow-dag-processor
    restart: always
    depends_on:
      airflow-api-server:
        condition: service_healthy
    env_file:
      - ./.env
      - ./docker/dags/.env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-postgres:5432/${AIRFLOW_DB_NAME}
      AIRFLOW__SDK__API_BASE_URL: http://airflow-api-server:8080
    volumes:
      - ./dags:/opt/airflow/dags
      - ./docker/logs:/opt/airflow/logs
      - ./docker/plugins:/opt/airflow/plugins
    command: dag-processor

  airflow-scheduler:
    build:
      context: .
      dockerfile: dockerfile-airflow.dockerfile
    container_name: airflow-scheduler
    restart: always
    depends_on:
      airflow-api-server:
        condition: service_healthy
    env_file:
      - ./.env
      - ./docker/dags/.env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-postgres:5432/${AIRFLOW_DB_NAME}
      AIRFLOW__SDK__API_BASE_URL: http://airflow-api-server:8080
    volumes:
      - ./dags:/opt/airflow/dags
      - ./docker/logs:/opt/airflow/logs
      - ./docker/plugins:/opt/airflow/plugins
    command: scheduler

volumes:
  airflow_postgres_data:
  postgres_data:
  minio_data:

networks:
  default:
    name: banking-mds-net


# services:
#   zookeeper:
#     image: confluentinc/cp-zookeeper:7.4.1
#     container_name: zookeeper
#     environment:
#       ZOOKEEPER_CLIENT_PORT: 2181
#       ZOOKEEPER_TICK_TIME: 2000
#     ports:
#       - "2181:2181"

#   kafka:
#     image: confluentinc/cp-kafka:7.4.1
#     container_name: kafka
#     depends_on:
#       - zookeeper
#     ports:
#       - "9092:9092"
#       - "29092:29092"
#     environment:
#       KAFKA_BROKER_ID: 1
#       KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
#       KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,PLAINTEXT_HOST://0.0.0.0:29092
#       KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://127.0.0.1:29092
#       KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
#       KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
#       KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
#       KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
#       KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1

#   postgres:
#     image: postgres:15
#     container_name: postgres
#     environment:
#       POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
#       POSTGRES_USER: ${POSTGRES_USER}
#       POSTGRES_DB: ${POSTGRES_DB}
#       POSTGRES_HOST_AUTH_METHOD: scram-sha-256
#       POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256"
#     ports:
#       - "5432:5432"
#     volumes:
#       - postgres_data:/var/lib/postgresql/data
#     command: >
#       postgres -c wal_level=logical
#               -c max_wal_senders=10
#               -c max_replication_slots=10
#               -c password_encryption=scram-sha-256
#     healthcheck:
#       test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
#       interval: 10s
#       timeout: 5s
#       retries: 5

#   connect:
#     image: quay.io/debezium/connect:3.4
#     container_name: connect
#     depends_on:
#       kafka:
#         condition: service_started
#       postgres:
#         condition: service_healthy
#     ports:
#       - "8083:8083"
#     environment:
#       BOOTSTRAP_SERVERS: "kafka:9092"
#       GROUP_ID: "1"
#       CONFIG_STORAGE_TOPIC: "connect-configs"
#       OFFSET_STORAGE_TOPIC: "connect-offsets"
#       STATUS_STORAGE_TOPIC: "connect-status"
#       CONFIG_STORAGE_REPLICATION_FACTOR: "1"
#       OFFSET_STORAGE_REPLICATION_FACTOR: "1"
#       STATUS_STORAGE_REPLICATION_FACTOR: "1"
#       KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
#       VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
#       KEY_CONVERTER_SCHEMAS_ENABLE: "false"
#       VALUE_CONVERTER_SCHEMAS_ENABLE: "false"

#   minio:
#     image: minio/minio:latest
#     container_name: minio
#     command: server /data --console-address ":9001"
#     environment:
#       MINIO_ROOT_USER: ${MINIO_ROOT_USER}
#       MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD}
#     ports:
#       - "9002:9000"
#       - "9001:9001"
#     volumes:
#       - minio_data:/data

#   airflow-postgres:
#     image: postgres:15
#     container_name: airflow-postgres
#     restart: always
#     env_file:
#       - ./docker/dags/.env
#     environment:
#       POSTGRES_USER: ${AIRFLOW_DB_USER}
#       POSTGRES_PASSWORD: ${AIRFLOW_DB_PASSWORD}
#       POSTGRES_DB: ${AIRFLOW_DB_NAME}
#     volumes:
#       - airflow_postgres_data:/var/lib/postgresql/data
#     ports:
#       - "5433:5432"
#     healthcheck:
#       test: ["CMD-SHELL", "pg_isready -U ${AIRFLOW_DB_USER} -d ${AIRFLOW_DB_NAME}"]
#       interval: 10s
#       timeout: 5s
#       retries: 10

#   airflow-init:
#     build:
#       context: .
#       dockerfile: dockerfile-airflow.dockerfile
#     container_name: airflow-init
#     depends_on:
#       airflow-postgres:
#         condition: service_healthy
#     env_file:
#       - ./docker/dags/.env
#     environment:
#       AIRFLOW__CORE__EXECUTOR: LocalExecutor
#       AIRFLOW__CORE__LOAD_EXAMPLES: "false"
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-postgres:5432/${AIRFLOW_DB_NAME}

#       # Auth manager
#       AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager

#       # ✅ IMPORTANT (keys)
#       AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
#       AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
#       AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}

#       # ✅ IMPORTANT (Airflow 3 execution API)
#       AIRFLOW__SDK__API_BASE_URL: http://airflow-api-server:8080

#       # admin user
#       AIRFLOW_ADMIN_USER: ${AIRFLOW_ADMIN_USER}
#       AIRFLOW_ADMIN_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD}
#       AIRFLOW_ADMIN_EMAIL: ${AIRFLOW_ADMIN_EMAIL}
#       AIRFLOW_ADMIN_FIRSTNAME: ${AIRFLOW_ADMIN_FIRSTNAME}
#       AIRFLOW_ADMIN_LASTNAME: ${AIRFLOW_ADMIN_LASTNAME}
#     volumes:
#       - ./docker/dags:/opt/airflow/dags
#       - ./docker/logs:/opt/airflow/logs
#       - ./docker/plugins:/opt/airflow/plugins
#     entrypoint: /bin/bash
#     command:
#       - -c
#       - |
#         set -e
#         airflow db migrate
#         airflow users create \
#           --username "${AIRFLOW_ADMIN_USER}" \
#           --password "${AIRFLOW_ADMIN_PASSWORD}" \
#           --firstname "${AIRFLOW_ADMIN_FIRSTNAME}" \
#           --lastname "${AIRFLOW_ADMIN_LASTNAME}" \
#           --role "Admin" \
#           --email "${AIRFLOW_ADMIN_EMAIL}" || true
#         echo "Airflow initialization completed!"
#     restart: "no"

#   airflow-api-server:
#     build:
#       context: .
#       dockerfile: dockerfile-airflow.dockerfile
#     container_name: airflow-api-server
#     restart: always
#     depends_on:
#       airflow-init:
#         condition: service_completed_successfully
#     env_file:
#       - ./docker/dags/.env
#     environment:
#       AIRFLOW__CORE__EXECUTOR: LocalExecutor
#       AIRFLOW__CORE__LOAD_EXAMPLES: "false"
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-postgres:5432/${AIRFLOW_DB_NAME}

#       AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
#       AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
#       AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
#       AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}

#       # ✅ Airflow 3 execution API base url (SDK client)
#       AIRFLOW__SDK__API_BASE_URL: http://airflow-api-server:8080

#       # (optional but fine)
#       AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.session
#     volumes:
#       - ./docker/dags:/opt/airflow/dags
#       - ./docker/logs:/opt/airflow/logs
#       - ./docker/plugins:/opt/airflow/plugins
#     ports:
#       - "8080:8080"
#     command: api-server
#     healthcheck:
#       test: ["CMD", "curl", "--fail", "http://localhost:8080/api/v2/monitor/health"]
#       interval: 30s
#       timeout: 10s
#       retries: 10

#   airflow-dag-processor:
#     build:
#       context: .
#       dockerfile: dockerfile-airflow.dockerfile
#     container_name: airflow-dag-processor
#     restart: always
#     depends_on:
#       airflow-api-server:
#         condition: service_healthy
#     env_file:
#       - ./docker/dags/.env
#     environment:
#       AIRFLOW__CORE__EXECUTOR: LocalExecutor
#       AIRFLOW__CORE__LOAD_EXAMPLES: "false"
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-postgres:5432/${AIRFLOW_DB_NAME}
#       AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
#       AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
#       AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
#       AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
#       AIRFLOW__SDK__API_BASE_URL: http://airflow-api-server:8080
#     volumes:
#       - ./docker/dags:/opt/airflow/dags
#       - ./docker/logs:/opt/airflow/logs
#       - ./docker/plugins:/opt/airflow/plugins
#     command: dag-processor

#   airflow-scheduler:
#     build:
#       context: .
#       dockerfile: dockerfile-airflow.dockerfile
#     container_name: airflow-scheduler
#     restart: always
#     depends_on:
#       airflow-api-server:
#         condition: service_healthy
#     env_file:
#       - ./docker/dags/.env
#     environment:
#       AIRFLOW__CORE__EXECUTOR: LocalExecutor
#       AIRFLOW__CORE__LOAD_EXAMPLES: "false"
#       AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@airflow-postgres:5432/${AIRFLOW_DB_NAME}
#       AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
#       AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
#       AIRFLOW__API__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}
#       AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__API__SECRET_KEY}

#       # ✅ THIS is the big missing piece in your setup
#       AIRFLOW__SDK__API_BASE_URL: http://airflow-api-server:8080
#     volumes:
#       - ./docker/dags:/opt/airflow/dags
#       - ./docker/logs:/opt/airflow/logs
#       - ./docker/plugins:/opt/airflow/plugins
#     command: scheduler

# volumes:
#   airflow_postgres_data:
#   postgres_data:
#   minio_data:

# networks:
#   default:
#     name: banking-mds-net
