# Banking Real-time Analytics Pipeline

A complete end-to-end real-time analytics pipeline for banking data, demonstrating modern data engineering practices with CDC, event streaming, data lake storage, and cloud data warehousing.

## ğŸ“‹ Project Overview

This project implements a production-grade data pipeline that:

- **Captures real-time changes** from a PostgreSQL banking database using Debezium CDC (Change Data Capture)
- **Streams events** through Apache Kafka for reliable, distributed message delivery
- **Stores raw data** in MinIO (S3-compatible object storage) as Parquet files, partitioned by date
- **Orchestrates workflows** using Apache Airflow to move data from the lake to warehouse
- **Loads data** into Snowflake data warehouse with VARIANT schema for flexibility
- **Transforms data** using dbt (data build tool) with staging, snapshots (SCD Type 2), and marts layers
- **Visualizes insights** through Power BI dashboard connected to the analytics layer
- **Automates testing and deployment** with GitHub Actions CI/CD pipelines

### Technologies Used

| Layer | Technology |
|-------|------------|
| Source Database | PostgreSQL |
| Change Data Capture | Debezium |
| Event Streaming | Apache Kafka |
| Data Lake | MinIO (S3-compatible) |
| File Format | Apache Parquet |
| Orchestration | Apache Airflow |
| Data Warehouse | Snowflake |
| Transformation | dbt (data build tool) |
| Visualization | Power BI |
| CI/CD | GitHub Actions |
| Containerization | Docker & Docker Compose |

## ğŸ—ï¸ Architecture

![Architecture Diagram](docs/architecture.png)

**Data Flow:**
1. Producer â†’ PostgreSQL (customers, accounts, transactions)
2. Debezium â†’ Kafka Topics (CDC events)
3. Consumer â†’ MinIO (Parquet files, partitioned by date)
4. Airflow â†’ Snowflake RAW (VARIANT columns)
5. dbt â†’ Analytics Layer (staging â†’ snapshots â†’ marts)
6. Power BI â†’ Dashboard

## ğŸ“Š Dashboard

![Power BI Dashboard](docs/dashboard_digram.png)

The Power BI dashboard provides real-time insights into:
- Customer analytics and segmentation
- Account performance metrics
- Transaction patterns and trends
- Historical data tracking with SCD Type 2

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.9+
- Snowflake account
- Git

### Setup

1. **Clone and configure**
```bash
git clone https://github.com/yourusername/banking-realtime-analytics.git
cd banking-realtime-analytics
cp .env.example .env
cp docker/dags/.env.example docker/dags/.env
```

2. **Generate Airflow secrets**
```bash
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
python -c "import secrets; print(secrets.token_urlsafe(64))"
```
Add these to `.env` file.

3. **Update Snowflake credentials** in `.env`

4. **Initialize Snowflake**
```sql
CREATE WAREHOUSE IF NOT EXISTS BANKING_WH;
CREATE DATABASE IF NOT EXISTS BANKING;
CREATE SCHEMA IF NOT EXISTS BANKING.RAW;

CREATE TABLE RAW.CUSTOMERS (DATA VARIANT, INGESTED_AT TIMESTAMP_NTZ);
CREATE TABLE RAW.ACCOUNTS (DATA VARIANT, INGESTED_AT TIMESTAMP_NTZ);
CREATE TABLE RAW.TRANSACTIONS (DATA VARIANT, INGESTED_AT TIMESTAMP_NTZ);
```

5. **Start services**
```bash
docker compose up -d --build
```

### Run Pipeline

```bash
# 1. Generate data
cd producer && python producer.py

# 2. Create CDC connector
cd ../connector && python create_connector.py --recreate

# 3. Consume to MinIO
cd ../consumer && python consumer.py

# 4. Trigger Airflow DAG at http://localhost:8080

# 5. Run dbt transformations
cd ../banking_dbt
dbt deps
dbt snapshot
dbt run
dbt test
```

### Access Services

| Service | URL | Credentials |
|---------|-----|-------------|
| Airflow | http://localhost:8080 | admin/admin |
| MinIO | http://localhost:9001 | minioadmin/minioadmin |
| Debezium | http://localhost:8083 | - |

## ğŸ“ Repository Structure

```
banking-realtime-analytics/
â”œâ”€â”€ .github/workflows/       # CI/CD pipelines
â”œâ”€â”€ banking_dbt/             # dbt project (staging, snapshots, marts)
â”œâ”€â”€ connector/               # Debezium connector setup
â”œâ”€â”€ consumer/                # Kafka â†’ MinIO consumer
â”œâ”€â”€ dags/                    # Airflow DAGs
â”œâ”€â”€ docs/                    # Architecture & dashboard diagrams
â”œâ”€â”€ producer/                # Data generator
â”œâ”€â”€ docker-compose.yml       # Infrastructure services
â””â”€â”€ README.md
```

## ğŸ§ª CI/CD

### Continuous Integration
- Linting with ruff
- Unit tests with pytest
- dbt model validation

### Continuous Deployment
- Automated dbt runs on push to main
- Snowflake deployment

**Required GitHub Secrets:**
- `SNOWFLAKE_ACCOUNT`
- `SNOWFLAKE_USER`
- `SNOWFLAKE_PASSWORD`
- `SNOWFLAKE_WAREHOUSE`

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file.

## ğŸ™ Acknowledgments

Built with Debezium, Kafka, MinIO, Airflow, Snowflake, dbt, and Power BI.

---
